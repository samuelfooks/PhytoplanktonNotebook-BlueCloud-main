{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create Monthly 3D Micro Nano and Pico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the PFT demonstrator?\n",
    "\n",
    "PFT_Product folder include three subfolders: Programs, Outputs, and Plots.\n",
    "\n",
    "The Programs folder contains 2 Jupyter notebooks and two folders (Functions and Models).\n",
    "The Functions folder contains all the necessary functions required to generate the 3D global products and the folder Models contains the trained MLP models and PCA models.\n",
    "The Outputs folder contains the output global 3D products generated for each month of the year 2018 (“.nc” files.)\n",
    "The Plots folder contains the visualization of the output products as “.png” files (2D spatial plots for 36 depths).\n",
    "\n",
    "The PFT_Product demonstrator can be executed in the Jupyter Lab of the VRE, by running the two Jupyter notebooks available in the “Phytoplankton_EOV/PFT_Product/Programs” folder, i.e. “CREATE_MONTHLY_FIELDS_PFT_ZNORM_N1.ipynb” and “Plots_output_spatial_monthly_PFT_2018.ipynb”. The first notebook generates the global 3D PFT products (Micro-Chla, Nano-Chla, and Pico-Chla) in NetCDF format. For each month, the output is saved in the corresponding monthly folder, under “Outputs”. The second notebook is used to generate the visualization plots based on the output NetCDF files obtained from the first notebook. For each month, the plots are saved in the corresponding monthly folder under “Plots”. Before executing these notebooks (“CREATE_MONTHLY_FIELDS_PFT_ZNORM_N1.ipynb”, “Plots_output_spatial_monthly_PFT_2018.ipynb”,  and “Functions/SOCA_PFT_2021_GLOBAL_N1.ipynb”), the paths should be checked and modified accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all functions that we need to run this demonstrator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset as NetCDFFile \n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "import oceans.sw_extras as swe\n",
    "import gsw\n",
    "from scipy.interpolate import interp1d\n",
    "import warnings\n",
    "# import oceans.sw_extras.sw_extras as swe\n",
    "import fnmatch\n",
    "import sys\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "warnings.filterwarnings('ignore') # Ignore warning messages for printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All function to create the 3D weekly NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/jovyan/PFT_Product/Programs/Functions/fonction_mask_black_sea.ipynb\n",
      "importing Jupyter notebook from /home/jovyan/PFT_Product/Programs/Functions/fonction_mask_bathymetry.ipynb\n",
      "importing Jupyter notebook from /home/jovyan/PFT_Product/Programs/Functions/Function_find_Ze_depth.ipynb\n",
      "importing Jupyter notebook from /home/jovyan/PFT_Product/Programs/Functions/SOCA_PFT_2021_GLOBAL_N1.ipynb\n",
      "importing Jupyter notebook from /home/jovyan/PFT_Product/Programs/Functions/fonction_NetCDF_monthly_N1.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Load functions\n",
    "import import_ipynb\n",
    "from Functions.fonction_mask_black_sea import mask_black_sea\n",
    "from Functions.fonction_mask_bathymetry import mask_bathy\n",
    "from Functions.Function_find_Ze_depth import find_Ze\n",
    "from Functions.SOCA_PFT_2021_GLOBAL_N1 import INPUTS_SOCA_PFT_GLOBAL_2021, SOCA_PFT_GLOBAL_2021\n",
    "from Functions.fonction_NetCDF_monthly_N1 import creation_NetCDF_3D_PRODUCT_CMEMS_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date and paths of Inputs and Output folder\n",
    "\n",
    "DATE_TODAY=\"02_12_2021\" # DATE_TODAY is the Notebook running date; the output files will be saved under the folder Output with names DATE_TODAY\n",
    "\n",
    "###############################################################################################\n",
    "# # To run in home folder please uncomment below paths\n",
    "path_TACMOB='/'.join(['/home/jovyan/Phytoplankton_EOV/PFT_Product/Outputs', DATE_TODAY])\n",
    "path_data_sla = '/home/jovyan/Phytoplankton_EOV/Inputs/SLA'\n",
    "path_data_rrs412 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS412'\n",
    "path_data_rrs443 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS443'\n",
    "path_data_rrs490 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS490'\n",
    "path_data_rrs555 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS555'\n",
    "path_data_rrs670 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS670'\n",
    "path_data_par = '/home/jovyan/Phytoplankton_EOV/Inputs/PAR'\n",
    "path_data_phy = '/home/jovyan/Phytoplankton_EOV/Inputs/ARMOR3D_N/'\n",
    "path_data_chla = '/home/jovyan/Phytoplankton_EOV/Inputs/CHLA'\n",
    "path_bathy_data=\"/home/jovyan/Phytoplankton_EOV/Inputs/BATHYMETRY/GEBCO_2014_6x6min_Global.nc\"\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "# # To run in WorkSpace VRE folder please uncomment below paths\n",
    "# path_TACMOB='/'.join(['/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/PFT_Product/Outputs', DATE_TODAY])\n",
    "# path_data_sla = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/SLA'\n",
    "# path_data_rrs412 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS412'\n",
    "# path_data_rrs443 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS443'\n",
    "# path_data_rrs490 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS490'\n",
    "# path_data_rrs555 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS555'\n",
    "# path_data_rrs670 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS670'\n",
    "# path_data_par = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/PAR'\n",
    "# path_data_phy = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/ARMOR3D_N/'\n",
    "# path_data_chla = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/CHLA'\n",
    "# path_bathy_data=\"/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/BATHYMETRY/GEBCO_2014_6x6min_Global.nc\"\n",
    "###############################################################################################\n",
    "\n",
    "# If the path where we will store the 3D NetCDF files is not a directory --> mkdir\n",
    "if not os.path.isdir(path_TACMOB):\n",
    "    os.mkdir(path_TACMOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important** - Input paths and FTP links to download monthly data\n",
    "#### In the demonstrator we show only for the year 2018 If user wish to produce PFT for an another year they can download it from the following links and keep in the Input folder\n",
    "\n",
    "| Variable | Path_folder | Original data dowloaded from |\n",
    "| ---| --- | --- |\n",
    "| RRS412 | `path_data_rrs412` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs412_4km_monthly-rep-v02/ |\n",
    "| RRS443 | `path_data_rrs443` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs443_4km_monthly-rep-v02/ |\n",
    "| RRS490 | `path_data_rrs490` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs490_4km_monthly-rep-v02/ |\n",
    "| RRS555 | `path_data_rrs555` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs555_4km_monthly-rep-v02/ |\n",
    "| RRS670 | `path_data_rrs670` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs670_4km_monthly-rep-v02/ |\n",
    "| PAR |  `path_data_par` | ftp://ftp.hermes.acri.fr/GLOB/merged/month/ |\n",
    "| SLA |  `path_data_sla` | ftp://my.cmems-du.eu/Core/SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047/dataset-duacs-rep-global-merged-allsat-phy-l4-monthly/ |\n",
    "| Physical_ARMOR3D |  `path_data_phy` | ftp://nrt.cmems-du.eu/Core/MULTIOBS_GLO_PHY_TSUV_3D_MYNRT_015_012/dataset-armor-3d-rep-monthly/ |\n",
    "| CHLA |  `path_data_chla` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs490_4km_monthly-rep-v02/  |\n",
    "| BATHYMETRY |  `path_bathy_data` | https://download.gebco.net/ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bathymetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_data=NetCDFFile(path_bathy_data)\n",
    "Height=bathy_data.variables['Height'][:]\n",
    "Lat_bathy=bathy_data.variables['lat'][:]\n",
    "Lon_bathy=bathy_data.variables['lon'][:]\n",
    "bathy_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to load a specific NetCDF file and return the matrix of parameter given as input with lon and lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load data in NetCDF files\n",
    "# Takes as input the path of the NetCDF file as well as the variable to return\n",
    "def load_netcdf(path, var):\n",
    "    \n",
    "    #Open the NetCDF file and get the Chl/lon and latitude data\n",
    "    nc=NetCDFFile(path)\n",
    "    \n",
    "    # Load data\n",
    "    data=nc.variables[str(var)][:]\n",
    "    \n",
    "    # Get 2D matrix\n",
    "    if len(data.shape)==4:\n",
    "        data=data[0,0,:,:]\n",
    "    if len(data.shape)==3:\n",
    "        data=data[0,:,:]\n",
    "        \n",
    "    if var == 'sla':\n",
    "        # SLA NetCDF have different variable names for longitude and latitude\n",
    "        lon_data=nc.variables['longitude'][:]\n",
    "        lon_data=list(lon_data)\n",
    "        lat_data=nc.variables['latitude'][:]\n",
    "        lat_data=list(lat_data)\n",
    "        # Longitude range between 0 and 360 instead of -180 to 180 --> transformation for range -180-180\n",
    "        for i in np.arange(len(lon_data)):\n",
    "            #print(i)\n",
    "            if lon_data[i]>=0 and lon_data[i]<180:\n",
    "                lon_data[i]=lon_data[i]\n",
    "            else:\n",
    "                lon_data[i]=lon_data[i]-360\n",
    "\n",
    "    else:\n",
    "        lon_data=nc.variables['lon'][:]\n",
    "        lon_data=list(lon_data)\n",
    "        lat_data=nc.variables['lat'][:]\n",
    "        lat_data=list(lat_data)\n",
    "    \n",
    "    \n",
    "    #Then, close the NetCDF file\n",
    "    nc.close()\n",
    "\n",
    "    #Order the longitude vector and the Chl matrix\n",
    "    sort_lon_data=np.argsort(lon_data)\n",
    "    data=data[:,sort_lon_data]\n",
    "    lon_data=np.array(lon_data)[sort_lon_data]\n",
    "\n",
    "    #Idem for latitude\n",
    "    sort_lat_data=np.argsort(lat_data)\n",
    "    data=data[sort_lat_data,:]\n",
    "    lat_data=np.array(lat_data)[sort_lat_data]\n",
    "\n",
    "    return data, lon_data, lat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that returns the mean from different pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Function to get the mean for different pixels in order to have the same resolution for satellite data as for physical data\n",
    "def get_mean_pixel(VAR_MAT, lon_ix, lat_ix):\n",
    "    #Take var alues for all iii and jjj --> mean value of these values is the matchup\n",
    "    VAR=VAR_MAT[np.ix_(lat_ix, lon_ix)]\n",
    "    #check if all values are masked: if yes put nan and if no compute the mean value\n",
    "    if len(VAR[~VAR.mask])==0:\n",
    "        MEAN=np.nan\n",
    "    else:\n",
    "        MEAN=np.nanmean(VAR[~VAR.mask])\n",
    "    return MEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that returns the ZNORM depth from MLD and Euphotic depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZNORM_N(chl_sat,mld):\n",
    "    Zeu = find_Ze(chl_sat)\n",
    "    ZNORM = max(1.5*(Zeu),mld)\n",
    "    return ZNORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "2\n",
      "dataset-armor-3d-rep-monthly_20180215T1200Z_P20201106T2226Z.nc\n",
      "/home/jovyan/Chla_Product/Inputs/RRS412/2018/20180201_m_20180228-ACRI-L4-RRS412-AVW_MULTI_4KM-GLO-REP-v02.nc\n",
      "/home/jovyan/Chla_Product/Inputs/RRS443/2018/20180201_m_20180228-ACRI-L4-RRS443-AVW_MULTI_4KM-GLO-REP-v02.nc\n",
      "/home/jovyan/Chla_Product/Inputs/RRS490/2018/20180201_m_20180228-ACRI-L4-RRS490-AVW_MULTI_4KM-GLO-REP-v02.nc\n",
      "/home/jovyan/Chla_Product/Inputs/RRS555/2018/20180201_m_20180228-ACRI-L4-RRS555-AVW_MULTI_4KM-GLO-REP-v02.nc\n",
      "/home/jovyan/Chla_Product/Inputs/RRS670/2018/20180201_m_20180228-ACRI-L4-RRS670-AVW_MULTI_4KM-GLO-REP-v02.nc\n",
      "/home/jovyan/Chla_Product/Inputs/CHLA/2018/20180201_m_20180228-ACRI-L4-CHL-MULTI_4KM-GLO-REP.nc\n",
      "/home/jovyan/Chla_Product/Inputs/PAR/2018/L3m_20180201-20180228__GLOB_4_AVW-MODVIR_PAR_MO_00.nc\n",
      "/home/jovyan/Chla_Product/Inputs/SLA/2018/dt_global_allsat_msla_h_y2018_m02.nc\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n"
     ]
    }
   ],
   "source": [
    "# vector depth contains all pressure of estimations (retrieval for the 3D cube, defined from ARMOR3D resolution)\n",
    "\n",
    "CMEMS_pres_N=[0, 5, 10, 15, 20, 25, 30, 35, 40,45, 50, 55, 60, 65, 70, 80, 90, 100,\n",
    "              125, 150, 175, 200, 225, 250, 275, 300, 350, 400, 450, 500, 550, 600,\n",
    "              700, 800, 900, 1000]\n",
    "\n",
    "y = 2018\n",
    "\n",
    "MONTHS=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "# By default this notebook generate monthly files for 12 months of year 2018; \n",
    "# If user want to generate the output for a special year, they can edit  MONTHS as follows \n",
    "# MONTHS=[1] \n",
    "# it generate product for January\n",
    "\n",
    "str_months = ['January', 'February', \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "for m in MONTHS:\n",
    "    print(y)\n",
    "    print(m) \n",
    "    M = str(m).zfill(2)\n",
    "    \n",
    "    #Create the subdirectory of path_TACMOB where the NetCDF of 3D Chl will be stored\n",
    "    dir_nc_year = '/'.join([path_TACMOB,str(y)])\n",
    "    if not os.path.isdir(dir_nc_year):\n",
    "        os.mkdir(dir_nc_year)\n",
    "    \n",
    "    dir_nc_tac_mob=[path_TACMOB,str(y),str(m)]\n",
    "    path_nc_tac_mob='/'.join(dir_nc_tac_mob)\n",
    "    if not os.path.isdir(path_nc_tac_mob):\n",
    "        os.mkdir(path_nc_tac_mob)\n",
    "        \n",
    "    dir_phy_data=[path_data_phy,str(y)]\n",
    "    path_phy='/'.join(dir_phy_data)\n",
    "    #go in this path to list every file with physical data (one file for one week of data)\n",
    "    os.chdir(path_phy)\n",
    "    nc_file_chemin_phy=os.listdir()\n",
    "    \n",
    "    #nc_file_chemin_phy=[sys.argv[2]]\n",
    "#     f = ''.join(['dataset-armor-3d-rep-monthly_',str(y),str(M),'15T1200Z_P20190301T0000Z.nc'])\n",
    "    FFF = ''.join([str(y),str(M),'15'])\n",
    "    f = ''.join([f1 for f1 in nc_file_chemin_phy if os.path.isfile(os.path.join(path_phy,f1)) and FFF in f1])\n",
    "#     ''.join(f)\n",
    "#     for f in nc_file_chemin_phy:\n",
    "    print(f)\n",
    "        \n",
    "    #Beginning time to compute the time to create one NetCDF\n",
    "    time1=datetime.now()\n",
    "        \n",
    "    #Get the date to have the corresponding ocean color data file\n",
    "    date=f.split(\"_\")[1][:8]\n",
    "        \n",
    "    #Get corresponding day of the year\n",
    "    date_format_datetime =  datetime.strptime(date,'%Y%m%d')\n",
    "    doy_f=float(date_format_datetime.strftime('%j'))\n",
    "        \n",
    "    path_NetCDF_phy=f\n",
    "    #open the NetCDF with physical data and get the MLD/lon/lat matrix\n",
    "    phy_data=NetCDFFile(path_NetCDF_phy)\n",
    "    #Get mixed layer depth\n",
    "    mlotst=phy_data.variables['mlotst'][:]\n",
    "    #[0,:,:] 0 because the 1st dimension of the 3D matrix was 1 --> so transformation in 2D matrix\n",
    "    mlotst=mlotst[0,:,:]\n",
    "    # Get temperature\n",
    "    to=phy_data.variables['to'][:]\n",
    "    #[0,0:19,:,:] 0 because the 1st dimension of the 4D matrix was 1 --> so transformation in 3D matrix\n",
    "    #0:19 to get the 19 depth for SOCA (from surface to 1000m depth)\n",
    "    to=to[0,0:36,:,:]\n",
    "    #Get salinity\n",
    "    so=phy_data.variables['so'][:]\n",
    "    #[0,0:19,:,:] 0 because the 1st dimension of the 4D matrix was 1 --> so transformation in 3D matrix\n",
    "    #0:19 to get the 19 depth for SOCA (from surface to 1000m depth)\n",
    "    so=so[0,0:36,:,:]\n",
    "    #Get lon and lat\n",
    "    lon_phy=phy_data.variables['longitude'][:]\n",
    "    lon_phy=list(lon_phy)\n",
    "    lat_phy=phy_data.variables['latitude'][:]\n",
    "    lat_phy=list(lat_phy)\n",
    "        \n",
    "    #close the NetCDF physical file\n",
    "    phy_data.close()      \n",
    "        \n",
    "    #transform longitude of physical file with range 0-360 with a new longitude with range -180 à 180 \n",
    "    #to have the same range of longitude between phy and ocean color data --> for the matchup\n",
    "    lon_phy2=lon_phy.copy()\n",
    "    for i in np.arange(len(lon_phy2)):\n",
    "            #print(i)\n",
    "        if lon_phy[i]>=0 and lon_phy[i]<180:\n",
    "            lon_phy2[i]=lon_phy2[i]\n",
    "        else:\n",
    "            lon_phy2[i]=lon_phy2[i]-360\n",
    "                \n",
    "    #Then order the longitude vector and the MLD/temp and sal matrix\n",
    "    sort_lon_phy2=np.argsort(lon_phy2)\n",
    "    mlotst=mlotst[:,sort_lon_phy2]\n",
    "    to=to[:,:,sort_lon_phy2]\n",
    "    so=so[:,:,sort_lon_phy2]\n",
    "    lon_phy2=np.array(lon_phy2)[sort_lon_phy2]\n",
    "        \n",
    "    #Get the satellite data path and files names (with the good year and date)\n",
    "    dir_data_rrs412='/'.join([path_data_rrs412,str(y)])\n",
    "    num_days = monthrange(y, m)\n",
    "    fin_day = num_days[1] # final day of the month       \n",
    "        \n",
    "    dir_data_rrs443='/'.join([path_data_rrs443,str(y)])\n",
    "    dir_data_rrs490='/'.join([path_data_rrs490,str(y)])\n",
    "    dir_data_rrs555='/'.join([path_data_rrs555,str(y)])\n",
    "    dir_data_rrs670='/'.join([path_data_rrs670,str(y)])\n",
    "    dir_data_chla='/'.join([path_data_chla,str(y)])\n",
    "               \n",
    "    dir_data_par='/'.join([path_data_par,str(y)])\n",
    "    dir_data_sla='/'.join([path_data_sla,str(y)])\n",
    "        \n",
    "               \n",
    "    path_NetCDF_rrs412=''.join([dir_data_rrs412,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS412-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs412)    \n",
    "        \n",
    "    path_NetCDF_rrs443=''.join([dir_data_rrs443,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS443-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs443)\n",
    "        \n",
    "    path_NetCDF_rrs490=''.join([dir_data_rrs490,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS490-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs490)\n",
    "        \n",
    "    path_NetCDF_rrs555=''.join([dir_data_rrs555,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS555-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs555)\n",
    "        \n",
    "    path_NetCDF_rrs670=''.join([dir_data_rrs670,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS670-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs670)\n",
    "        \n",
    "    path_NetCDF_chla=''.join([dir_data_chla,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-CHL-MULTI_4KM-GLO-REP.nc'])\n",
    "    print(path_NetCDF_chla)  \n",
    "        \n",
    "    path_NetCDF_par =''.join([dir_data_par,'/L3m_',str(y),str(M),'01-',str(y),str(M),str(fin_day),'__GLOB_4_AVW-MODVIR_PAR_MO_00.nc'])\n",
    "    print(path_NetCDF_par)         \n",
    "        \n",
    "    path_NetCDF_sla=''.join([dir_data_sla,'/', 'dt_global_allsat_msla_h_y', str(y),'_m',str(M),'.nc'])\n",
    "    print(path_NetCDF_sla)\n",
    "    #####################################################################\n",
    "    \n",
    "    rrs412 = load_netcdf(path=path_NetCDF_rrs412, var=\"RRS412\")[0]\n",
    "    lon_oc = load_netcdf(path=path_NetCDF_rrs412, var=\"RRS412\")[1]\n",
    "    lat_oc = load_netcdf(path=path_NetCDF_rrs412, var=\"RRS412\")[2]\n",
    "    \n",
    "    rrs443 = load_netcdf(path=path_NetCDF_rrs443, var=\"RRS443\")[0]\n",
    "       \n",
    "    rrs490 = load_netcdf(path=path_NetCDF_rrs490, var=\"RRS490\")[0]\n",
    "        \n",
    "    rrs555 = load_netcdf(path=path_NetCDF_rrs555, var=\"RRS555\")[0]\n",
    "        \n",
    "    rrs670 = load_netcdf(path=path_NetCDF_rrs670, var=\"RRS670\")[0]       \n",
    "    \n",
    "    chla = load_netcdf(path=path_NetCDF_chla, var=\"CHL\")[0]        \n",
    "    \n",
    "    par = load_netcdf(path=path_NetCDF_par, var=\"PAR_mean\")[0]\n",
    "    \n",
    "    \n",
    "    sla = load_netcdf(path=path_NetCDF_sla, var=\"sla\")[0]\n",
    "    lon_sla = load_netcdf(path=path_NetCDF_sla, var=\"sla\")[1]\n",
    "    lat_sla = load_netcdf(path=path_NetCDF_sla, var=\"sla\")[2]\n",
    "        \n",
    "    \n",
    "       \n",
    "        \n",
    "    #Create VAR3D matrix which is the matrix in which we will have the cube retrieval of Chl \n",
    "    TChla_3D=np.empty((len(CMEMS_pres_N),len(lat_phy),len(lon_phy2)))\n",
    "    TChla_3D[:,:,:]=np.nan\n",
    "    #Create CHL3D ERROR matrix which is the matrix in which we will have the cube error of retrieval of Chl\n",
    "    TChla_3D_ERR=np.empty((len(CMEMS_pres_N),len(lat_phy),len(lon_phy2)))\n",
    "    TChla_3D_ERR[:,:,:]=np.nan\n",
    "    \n",
    "    Micro_3D = TChla_3D.copy()\n",
    "    Micro_3D_ERR = TChla_3D_ERR.copy()\n",
    "    Nano_3D = TChla_3D.copy()\n",
    "    Nano_3D_ERR = TChla_3D_ERR.copy()\n",
    "    Pico_3D = TChla_3D.copy()\n",
    "    Pico_3D_ERR = TChla_3D_ERR.copy()\n",
    "    \n",
    "\n",
    "    #Loop for each pixel (lon/lat phy)\n",
    "#     for i in np.arange(len(MONTHS)):\n",
    "    for i in np.arange(len(lon_phy2)):\n",
    "        print(i)\n",
    "        warnings.filterwarnings('ignore') # Ignore warning messages for printing\n",
    "        #iii_oc the location of the pixel of ocean color that match with physical longitude\n",
    "        iii_oc=np.logical_and(lon_oc >= lon_phy2[i]-0.125, lon_oc <= lon_phy2[i]+0.125)\n",
    "        iii_oc=np.where(iii_oc)\n",
    "        iii_oc=np.array(iii_oc)[0,:]\n",
    "            \n",
    "        #iii_sla is the location of the pixel of sla that match with physical longitude\n",
    "        iii_sla=np.logical_and(lon_sla >= lon_phy2[i]-0.125, lon_sla <= lon_phy2[i]+0.125)\n",
    "        iii_sla=np.where(iii_sla)\n",
    "        iii_sla=np.array(iii_sla)[0,:]\n",
    "\n",
    "        for j in np.arange(len(lat_phy)):\n",
    "            #jjj_oc is the location of the pixel of ocean color that match with physical latitude\n",
    "            jjj_oc=np.logical_and(lat_oc >= lat_phy[j]-0.125, lat_oc <= lat_phy[j]+0.125)\n",
    "            jjj_oc=np.where(jjj_oc)\n",
    "            jjj_oc=np.array(jjj_oc)[0,:]\n",
    "                \n",
    "            #jjj_sla is the location of the pixel of sla that match with physical latitude\n",
    "            jjj_sla=np.logical_and(lat_sla >= lat_phy[j]-0.125, lat_sla <= lat_phy[j]+0.125)\n",
    "            jjj_sla=np.where(jjj_sla)\n",
    "            jjj_sla=np.array(jjj_sla)[0,:]\n",
    "                \n",
    "            # Apply the Black Sea mask:\n",
    "            black_sea=mask_black_sea(lon=lon_phy2[i], lat=lat_phy[j])   \n",
    "            # Apply the bathymetric mask <ith threshold 1500m depth:\n",
    "            bathy=mask_bathy(bathy_mat=Height, lon_bathy=Lon_bathy, lat_bathy=Lat_bathy, lon=lon_phy2[i], lat=lat_phy[j], threshold=-1500)\n",
    "            if not black_sea and bathy:  \n",
    "#                    print(\"ok\")\n",
    "                #Get the MLD and Chl value for Chl vertical distribution\n",
    "                mld_soca=mlotst[j,i]\n",
    "#                     chl_sat_uitz=get_mean_pixel(VAR_MAT=chl, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "\n",
    "                # Get the temperature and salinity profiles\n",
    "                temp_soca = to[:,j,i]\n",
    "                sal_soca = so[:,j,i]\n",
    "                \n",
    "             \n",
    "                # Get the RRS values and SLA and PAR\n",
    "                rrs412_soca=get_mean_pixel(VAR_MAT=rrs412, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs443_soca=get_mean_pixel(VAR_MAT=rrs443, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs490_soca=get_mean_pixel(VAR_MAT=rrs490, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs555_soca=get_mean_pixel(VAR_MAT=rrs555, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs670_soca=get_mean_pixel(VAR_MAT=rrs670, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                par_soca=get_mean_pixel(VAR_MAT=par, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                sla_soca=get_mean_pixel(VAR_MAT=sla, lon_ix=iii_sla, lat_ix=jjj_sla)\n",
    "                chla_matchup = get_mean_pixel(VAR_MAT=chla, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                ZNORM=ZNORM_N(chla_matchup,mld_soca)\n",
    "\n",
    "                    # Condition also on temp and sal because sometimes some depths are masked and not others so mld is not nan but some values in the T/S profiles are\n",
    "#                     isinstance(mld_uitz,float) and\n",
    "                if isinstance(mld_soca,float) and all([isinstance(T, float) for T in temp_soca]) and all([isinstance(S, float) for S in sal_soca]):\n",
    "                    if not np.isnan(rrs412_soca) and not np.isnan(ZNORM) and not np.isnan(mld_soca) and not np.isnan(rrs443_soca) and not np.isnan(rrs490_soca) and not np.isnan(rrs555_soca) and not np.isnan(rrs670_soca) and not np.isnan(sla_soca) and not np.isnan(par_soca):\n",
    "                        \n",
    "                        ############ DERIVE SPICINESS ######################################\n",
    "                        spici_soca = swe.spice(pd.DataFrame(sal_soca), pd.DataFrame(temp_soca), pd.DataFrame(CMEMS_pres_N))\n",
    "                        spici_soca = np.squeeze(spici_soca)      \n",
    "                        Zeta_interp = np.linspace(0,1.5,50)\n",
    "                        Zeta = CMEMS_pres_N/ZNORM\n",
    "                        temp_soca_ZNORM = np.interp(Zeta_interp, Zeta,temp_soca)\n",
    "                        sal_soca_ZNORM = np.interp(Zeta_interp, Zeta,sal_soca)\n",
    "                        spici_soca_ZNORM = np.interp(Zeta_interp, Zeta,spici_soca)          \n",
    "                        temp_soca_ZNORM = temp_soca_ZNORM.reshape(-1,1)\n",
    "                        sal_soca_ZNORM = sal_soca_ZNORM.reshape(-1,1)\n",
    "                        spici_soca_ZNORM = spici_soca_ZNORM.reshape(-1,1)\n",
    "                        \n",
    "#                         Model_type = Global_regional_model(lat=int(lat_phy[j]), lon=int(lon_phy2[i]))\n",
    "#                         if (Model_type == 'Global model'):                    \n",
    "                    \n",
    "                        soca_pft_2021_global_inputs=INPUTS_SOCA_PFT_GLOBAL_2021(RHO_WN_412=rrs412_soca*3.14, RHO_WN_443= rrs443_soca*3.14, RHO_WN_490=rrs490_soca*3.14,\n",
    "                                                                                  RHO_WN_555= rrs555_soca*3.14, RHO_WN_670= rrs670_soca*3.14, SLA= sla_soca, PAR=par_soca,\n",
    "                                                                                  MLD=mld_soca,sal= sal_soca_ZNORM, temp=temp_soca_ZNORM, spici=spici_soca_ZNORM,\n",
    "                                                                                  lon=lon_phy2[i],lat=lat_phy[j], doy=doy_f)                        \n",
    "                        \n",
    "                        PFT_N = SOCA_PFT_GLOBAL_2021(soca_pft_2021_global_inputs,ZNORM,pres_new=CMEMS_pres_N)\n",
    "                        PFT = pd.DataFrame(PFT_N)\n",
    "                \n",
    "\n",
    "                        Micro_3D[:,j,i]=PFT.iloc[0,:]\n",
    "                        Micro_3D_ERR[:,j,i]=PFT.iloc[1,:]\n",
    "                        \n",
    "                        Nano_3D[:,j,i]=PFT.iloc[2,:]\n",
    "                        Nano_3D_ERR[:,j,i]=PFT.iloc[3,:]\n",
    "                        \n",
    "                        Pico_3D[:,j,i]=PFT.iloc[4,:]\n",
    "                        Pico_3D_ERR[:,j,i]=PFT.iloc[5,:]\n",
    "                        \n",
    "\n",
    "\n",
    "            \n",
    "    date1=f.split(\"_\")[1][:8]\n",
    "    DATE_PRODUCT=date1\n",
    "    LONG=lon_phy2\n",
    "    print(LONG.shape)\n",
    "    LAT=np.array(lat_phy,)\n",
    "    print(LAT.shape)\n",
    "    DEPTH=np.array(CMEMS_pres_N)\n",
    "    DEPTH=np.array(DEPTH,dtype=\"int16\")\n",
    "    print(DEPTH.shape)\n",
    "    \n",
    "    VAR4D1=np.expand_dims(Micro_3D, axis=0)\n",
    "    print(VAR4D1.shape)\n",
    "    VAR4D_ERR1=np.expand_dims(Micro_3D_ERR, axis=0)\n",
    "    print(VAR4D_ERR1.shape)\n",
    "    \n",
    "    VAR4D2=np.expand_dims(Nano_3D, axis=0)\n",
    "    print(VAR4D2.shape)\n",
    "    VAR4D_ERR2=np.expand_dims(Nano_3D_ERR, axis=0)\n",
    "    print(VAR4D_ERR2.shape)\n",
    "    \n",
    "    VAR4D3=np.expand_dims(Pico_3D, axis=0)\n",
    "    print(VAR4D3.shape)\n",
    "    VAR4D_ERR3=np.expand_dims(Pico_3D_ERR, axis=0)\n",
    "    print(VAR4D_ERR3.shape)\n",
    "    \n",
    "\n",
    "    #Creation of the NetCDF\n",
    "    creation_NetCDF_3D_PRODUCT_CMEMS_N(path=path_nc_tac_mob, date_today= DATE_TODAY, date_product=DATE_PRODUCT,depth_input=DEPTH,longitude_input=LONG, latitude_input=LAT,\n",
    "                                       variable_4d_input1=VAR4D1, variable_4d_error_input1=VAR4D_ERR1, variable_4d_input2=VAR4D2, variable_4d_error_input2=VAR4D_ERR2,\n",
    "                                       variable_4d_input3=VAR4D3, variable_4d_error_input3=VAR4D_ERR3) \n",
    "#                                          variable_4d_input2=VAR4D2,variable_4d_error_input2=VAR4D_ERR2)\n",
    "        #End time to compute the time to create the NetCDF\n",
    "    time2=datetime.now()\n",
    "    #Compute the time\n",
    "    difference =time2-time1\n",
    "    print(difference)\n",
    "#     text_file = open(''.join([str(path_TACMOB),str(f),'_time_computation.txt']), \"w\")\n",
    "#     n = text_file.write(str(difference))\n",
    "#     text_file.close()\n",
    "########################\n",
    "########################       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
