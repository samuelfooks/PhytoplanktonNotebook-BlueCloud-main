{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create Monthly 3D SOCA-Chl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the Chla demonstrator?\n",
    "\n",
    "Chla_Product folder include three subfolders: Programs, Outputs, and Plots.\n",
    "\n",
    "The Programs folder contains 2 Jupyter notebooks and two folders (Functions and Models).\n",
    "The Functions folder contains all the necessary functions required to generate the 3D global products and the folder Models contains the trained MLP models and PCA models.\n",
    "The Outputs folder contains the output global 3D products generated for each month of the year 2018 (“.nc” files.)\n",
    "The Plots folder contains the visualization of the output products as “.png” files (2D spatial plots for 36 depths).\n",
    "\n",
    "The “Chla_Product” demonstrator can be executed in the Jupyter Lab of the VRE, by running the two Jupyter notebooks available in the “Phytoplankton_EOV/Chla_Product/Programs” folder, i.e. “CREATE_MONTHLY_FIELDS_Loop_ZNORM.ipynb” and “Output_spatial_plots.ipynb”. The first notebook generates the global 3D Chla products in NetCDF format. For each month, the output is saved in the corresponding monthly folder, under “Outputs”. The second notebook is used to generate the visualization plots based on the output NetCDF files obtained from the first notebook. For each month, the plots are saved in the corresponding monthly folder under “Plots”. Before executing these notebooks (“CREATE_MONTHLY_FIELDS_Loop_ZNORM.ipynb”, “Output_spatial_plots.ipynb”, and “Functions/SOCA_CHLA_ZNORM_2020.ipynb”), the paths should be checked and modified accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all functions that we need\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset as NetCDFFile \n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "import oceans.sw_extras as swe\n",
    "import gsw\n",
    "from scipy.interpolate import interp1d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Ignore warning messages for printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the 3D Monthly NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Chla_Product/Programs/Functions/fonction_mask_black_sea.ipynb\n",
      "importing Jupyter notebook from /workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Chla_Product/Programs/Functions/fonction_mask_bathymetry.ipynb\n",
      "importing Jupyter notebook from /workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Chla_Product/Programs/Functions/Function_find_Ze_depth.ipynb\n",
      "importing Jupyter notebook from /workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Chla_Product/Programs/Functions/SOCA_CHLA_ZNORM_2020.ipynb\n",
      "importing Jupyter notebook from /workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Chla_Product/Programs/Functions/fonction_NetCDF_monthly_N.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Functions.fonction_mask_black_sea import mask_black_sea\n",
    "from Functions.fonction_mask_bathymetry import mask_bathy\n",
    "from Functions.Function_find_Ze_depth import find_Ze\n",
    "from Functions.SOCA_CHLA_ZNORM_2020 import INPUTS_SOCA_CHLA_2020, SOCA_CHLA_ZNORM_2020\n",
    "from Functions.fonction_NetCDF_monthly_N import creation_NetCDF_3D_PRODUCT_CMEMS_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for inputs and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date and paths\n",
    "# DATE_TODAY=\"20210222\"\n",
    "DATE_TODAY = '20211201' # DATE_TODAY is the Notebook running date; the output files will be saved under the folder Output with names DATE_TODAY\n",
    "\n",
    "###############################################################################################\n",
    "# # To run in home folder please uncomment below paths\n",
    "path_TACMOB='/'.join(['/home/jovyan/Phytoplankton_EOV/Chla_Product/Outputs', DATE_TODAY])\n",
    "path_data_sla = '/home/jovyan/Phytoplankton_EOV/Inputs/SLA'\n",
    "path_data_rrs412 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS412'\n",
    "path_data_rrs443 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS443'\n",
    "path_data_rrs490 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS490'\n",
    "path_data_rrs555 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS555'\n",
    "path_data_rrs670 = '/home/jovyan/Phytoplankton_EOV/Inputs/RRS670'\n",
    "path_data_par = '/home/jovyan/Phytoplankton_EOV/Inputs/PAR'\n",
    "path_data_phy = '/home/jovyan/Phytoplankton_EOV/Inputs/ARMOR3D_N/'\n",
    "path_data_chla = '/home/jovyan/Phytoplankton_EOV/Inputs/CHLA'\n",
    "path_bathy_data=\"/home/jovyan/Phytoplankton_EOV/Inputs/BATHYMETRY/GEBCO_2014_6x6min_Global.nc\"\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "# # To run in WorkSpace VRE folder please uncomment below paths\n",
    "# path_TACMOB='/'.join(['/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Chla_Product/Outputs', DATE_TODAY])\n",
    "# path_data_sla = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/SLA'\n",
    "# path_data_rrs412 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS412'\n",
    "# path_data_rrs443 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS443'\n",
    "# path_data_rrs490 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS490'\n",
    "# path_data_rrs555 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS555'\n",
    "# path_data_rrs670 = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/RRS670'\n",
    "# path_data_par = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/PAR'\n",
    "# path_data_phy = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/ARMOR3D_N/'\n",
    "# path_data_chla = '/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/CHLA'\n",
    "# path_bathy_data=\"/workspace/VREFolders/Zoo-Phytoplankton_EOV/Phytoplankton_EOV/Inputs/BATHYMETRY/GEBCO_2014_6x6min_Global.nc\"\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "# If the path where we will store the 3D NetCDF files is not a directory --> mkdir\n",
    "if not os.path.isdir(path_TACMOB):\n",
    "    os.mkdir(path_TACMOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important** - Input paths and FTP links to download monthly data\n",
    "\n",
    "| Variable | Path_folder | Original data dowloaded from |\n",
    "| ---| --- | --- |\n",
    "| RRS412 | `path_data_rrs412` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs412_4km_monthly-rep-v02/ |\n",
    "| RRS443 | `path_data_rrs443` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs443_4km_monthly-rep-v02/ |\n",
    "| RRS490 | `path_data_rrs490` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs490_4km_monthly-rep-v02/ |\n",
    "| RRS555 | `path_data_rrs555` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs555_4km_monthly-rep-v02/ |\n",
    "| RRS670 | `path_data_rrs670` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs670_4km_monthly-rep-v02/ |\n",
    "| PAR |  `path_data_par` | ftp://ftp.hermes.acri.fr/GLOB/merged/month/ |\n",
    "| SLA |  `path_data_sla` | ftp://my.cmems-du.eu/Core/SEALEVEL_GLO_PHY_L4_REP_OBSERVATIONS_008_047/dataset-duacs-rep-global-merged-allsat-phy-l4-monthly/ |\n",
    "| Physical_ARMOR3D |  `path_data_phy` | ftp://nrt.cmems-du.eu/Core/MULTIOBS_GLO_PHY_TSUV_3D_MYNRT_015_012/dataset-armor-3d-rep-monthly/ |\n",
    "| CHLA |  `path_data_chla` | ftp://my.cmems-du.eu/Core/OCEANCOLOUR_GLO_OPTICS_L4_REP_OBSERVATIONS_009_081/dataset-oc-glo-opt-multi-l4-rrs490_4km_monthly-rep-v02/  |\n",
    "| BATHYMETRY |  `path_bathy_data` | https://download.gebco.net/ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bathymetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_data=NetCDFFile(path_bathy_data)\n",
    "Height=bathy_data.variables['Height'][:]\n",
    "Lat_bathy=bathy_data.variables['lat'][:]\n",
    "Lon_bathy=bathy_data.variables['lon'][:]\n",
    "bathy_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to load a specific NetCDF file and return the matrix of parameter given as input with lon and lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load data in NetCDF files\n",
    "# Takes as input the path of the NetCDF file as well as the variable to return\n",
    "def load_netcdf(path, var):\n",
    "    \n",
    "    #Open the NetCDF file and get the Chl/lon and latitude data\n",
    "    nc=NetCDFFile(path)\n",
    "    \n",
    "    # Load data\n",
    "    data=nc.variables[str(var)][:]\n",
    "    \n",
    "    # Get 2D matrix\n",
    "    if len(data.shape)==4:\n",
    "        data=data[0,0,:,:]\n",
    "    if len(data.shape)==3:\n",
    "        data=data[0,:,:]\n",
    "        \n",
    "    if var == 'sla':\n",
    "        # SLA NetCDF have different variable names for longitude and latitude\n",
    "        lon_data=nc.variables['longitude'][:]\n",
    "        lon_data=list(lon_data)\n",
    "        lat_data=nc.variables['latitude'][:]\n",
    "        lat_data=list(lat_data)\n",
    "        # Longitude range between 0 and 360 instead of -180 to 180 --> transformation for range -180-180\n",
    "        for i in np.arange(len(lon_data)):\n",
    "            #print(i)\n",
    "            if lon_data[i]>=0 and lon_data[i]<180:\n",
    "                lon_data[i]=lon_data[i]\n",
    "            else:\n",
    "                lon_data[i]=lon_data[i]-360\n",
    "\n",
    "    else:\n",
    "        lon_data=nc.variables['lon'][:]\n",
    "        lon_data=list(lon_data)\n",
    "        lat_data=nc.variables['lat'][:]\n",
    "        lat_data=list(lat_data)\n",
    "    \n",
    "    \n",
    "    #Then, close the NetCDF file\n",
    "    nc.close()\n",
    "\n",
    "    #Order the longitude vector and the Chl matrix\n",
    "    sort_lon_data=np.argsort(lon_data)\n",
    "    data=data[:,sort_lon_data]\n",
    "    lon_data=np.array(lon_data)[sort_lon_data]\n",
    "\n",
    "    #Idem for latitude\n",
    "    sort_lat_data=np.argsort(lat_data)\n",
    "    data=data[sort_lat_data,:]\n",
    "    lat_data=np.array(lat_data)[sort_lat_data]\n",
    "\n",
    "    return data, lon_data, lat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that returns the mean from different pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the mean for different pixels in order to have the same resolution for satellite data as for physical data\n",
    "def get_mean_pixel(VAR_MAT, lon_ix, lat_ix):\n",
    "    #Take var alues for all iii and jjj --> mean value of these values is the matchup\n",
    "    VAR=VAR_MAT[np.ix_(lat_ix, lon_ix)]\n",
    "    #check if all values are masked: if yes put nan and if no compute the mean value\n",
    "    if len(VAR[~VAR.mask])==0:\n",
    "        MEAN=np.nan\n",
    "    else:\n",
    "        MEAN=np.nanmean(VAR[~VAR.mask])\n",
    "    return MEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ZNORM depth from surface Chla and MLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZNORM_N(chl_sat,mld):\n",
    "    Zeu = find_Ze(chl_sat)\n",
    "    ZNORM = max(1.5*(Zeu),mld)\n",
    "    return ZNORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly .nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "1\n",
      "dataset-armor-3d-rep-monthly_20180115T1200Z_P20201106T2226Z.nc\n"
     ]
    }
   ],
   "source": [
    "#vector depth contains all pressure of estimations (retrieval for the 3D cube, defined from ARMOR3D resolution)\n",
    "\n",
    "CMEMS_pres_N=[0, 5, 10, 15, 20, 25, 30, 35, 40,45, 50, 55, 60, 65, 70, 80, 90, 100,\n",
    "              125, 150, 175, 200, 225, 250, 275, 300, 350, 400, 450, 500, 550, 600,\n",
    "              700, 800, 900, 1000]\n",
    "y = 2018\n",
    "MONTHS=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "# By default this notebook generate monthly files for 12 months of year 2018; \n",
    "# If user want to generate the output for a special year, they can edit  MONTHS as follows \n",
    "# MONTHS=[1] \n",
    "# it generate product for January\n",
    "\n",
    "str_months = ['January', 'February', \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "for m in MONTHS:\n",
    "    print(y)\n",
    "    print(m) \n",
    "    M = str(m).zfill(2)\n",
    "    \n",
    "    #Create the subdirectory of path_TACMOB where the NetCDF of 3D Chl will be stored\n",
    "    \n",
    "    dir_out_year = '/'.join([path_TACMOB,str(y)])\n",
    "    if not os.path.isdir(dir_out_year):\n",
    "        os.mkdir(dir_out_year)\n",
    "    \n",
    "    dir_nc_tac_mob=[path_TACMOB,str(y),str(m)]\n",
    "    path_nc_tac_mob='/'.join(dir_nc_tac_mob)\n",
    "    if not os.path.isdir(path_nc_tac_mob):\n",
    "        os.mkdir(path_nc_tac_mob)\n",
    "        \n",
    "    dir_phy_data=[path_data_phy,str(y)]\n",
    "    path_phy='/'.join(dir_phy_data)\n",
    "    #go in this path to list every file with physical data (one file for one week of data)\n",
    "    os.chdir(path_phy)\n",
    "    nc_file_chemin_phy=os.listdir()\n",
    "    \n",
    "    #nc_file_chemin_phy=[sys.argv[2]]\n",
    "#     f = ''.join(['dataset-armor-3d-rep-monthly_',str(y),str(M),'15T1200Z_P20190301T0000Z.nc'])\n",
    "#     for f in nc_file_chemin_phy:\n",
    "#     print(f)\n",
    "    \n",
    "    FFF = ''.join([str(y),str(M),'15'])\n",
    "    f = ''.join([f1 for f1 in nc_file_chemin_phy if os.path.isfile(os.path.join(path_phy,f1)) and FFF in f1])\n",
    "#     ''.join(f)\n",
    "#     for f in nc_file_chemin_phy:\n",
    "    print(f)\n",
    "        \n",
    "    #Beginning time to compute the time to create one NetCDF\n",
    "    time1=datetime.now()\n",
    "        \n",
    "    #Get the date to have the corresponding ocean color data file\n",
    "    date=f.split(\"_\")[1][:8]\n",
    "        \n",
    "    #Get corresponding day of the year\n",
    "    date_format_datetime =  datetime.strptime(date,'%Y%m%d')\n",
    "    doy_f=float(date_format_datetime.strftime('%j'))\n",
    "        \n",
    "    path_NetCDF_phy=f\n",
    "    #open the NetCDF with physical data and get the MLD/lon/lat matrix\n",
    "    phy_data=NetCDFFile(path_NetCDF_phy)\n",
    "    #Get mixed layer depth\n",
    "    mlotst=phy_data.variables['mlotst'][:]\n",
    "    #[0,:,:] 0 because the 1st dimension of the 3D matrix was 1 --> so transformation in 2D matrix\n",
    "    mlotst=mlotst[0,:,:]\n",
    "    # Get temperature\n",
    "    to=phy_data.variables['to'][:]\n",
    "    #[0,0:19,:,:] 0 because the 1st dimension of the 4D matrix was 1 --> so transformation in 3D matrix\n",
    "    #0:19 to get the 19 depth for SOCA (from surface to 1000m depth)\n",
    "    to=to[0,0:36,:,:]\n",
    "    #Get salinity\n",
    "    so=phy_data.variables['so'][:]\n",
    "    #[0,0:19,:,:] 0 because the 1st dimension of the 4D matrix was 1 --> so transformation in 3D matrix\n",
    "    #0:19 to get the 19 depth for SOCA (from surface to 1000m depth)\n",
    "    so=so[0,0:36,:,:]\n",
    "    #Get lon and lat\n",
    "    lon_phy=phy_data.variables['longitude'][:]\n",
    "    lon_phy=list(lon_phy)\n",
    "    lat_phy=phy_data.variables['latitude'][:]\n",
    "    lat_phy=list(lat_phy)\n",
    "        \n",
    "    #close the NetCDF physical file\n",
    "    phy_data.close()      \n",
    "        \n",
    "    #transform longitude of physical file with range 0-360 with a new longitude with range -180 à 180 \n",
    "    #to have the same range of longitude between phy and ocean color data --> for the matchup\n",
    "    lon_phy2=lon_phy.copy()\n",
    "    for i in np.arange(len(lon_phy2)):\n",
    "            #print(i)\n",
    "        if lon_phy[i]>=0 and lon_phy[i]<180:\n",
    "            lon_phy2[i]=lon_phy2[i]\n",
    "        else:\n",
    "            lon_phy2[i]=lon_phy2[i]-360\n",
    "                \n",
    "    #Then order the longitude vector and the MLD/temp and sal matrix\n",
    "    sort_lon_phy2=np.argsort(lon_phy2)\n",
    "    mlotst=mlotst[:,sort_lon_phy2]\n",
    "    to=to[:,:,sort_lon_phy2]\n",
    "    so=so[:,:,sort_lon_phy2]\n",
    "    lon_phy2=np.array(lon_phy2)[sort_lon_phy2]\n",
    "        \n",
    "    #Get the satellite data path and files names (with the good year and date)\n",
    "    dir_data_rrs412='/'.join([path_data_rrs412,str(y)])\n",
    "    num_days = monthrange(y, m)\n",
    "    fin_day = num_days[1] # final day of the month       \n",
    "        \n",
    "    dir_data_rrs443='/'.join([path_data_rrs443,str(y)])\n",
    "    dir_data_rrs490='/'.join([path_data_rrs490,str(y)])\n",
    "    dir_data_rrs555='/'.join([path_data_rrs555,str(y)])\n",
    "    dir_data_rrs670='/'.join([path_data_rrs670,str(y)])\n",
    "    dir_data_chla='/'.join([path_data_chla,str(y)])\n",
    "               \n",
    "    dir_data_par='/'.join([path_data_par,str(y)])\n",
    "    dir_data_sla='/'.join([path_data_sla,str(y)])\n",
    "        \n",
    "               \n",
    "    path_NetCDF_rrs412=''.join([dir_data_rrs412,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS412-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs412)    \n",
    "        \n",
    "    path_NetCDF_rrs443=''.join([dir_data_rrs443,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS443-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs443)\n",
    "        \n",
    "    path_NetCDF_rrs490=''.join([dir_data_rrs490,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS490-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs490)\n",
    "        \n",
    "    path_NetCDF_rrs555=''.join([dir_data_rrs555,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS555-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs555)\n",
    "        \n",
    "    path_NetCDF_rrs670=''.join([dir_data_rrs670,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-RRS670-AVW_MULTI_4KM-GLO-REP-v02.nc'])\n",
    "    print(path_NetCDF_rrs670)\n",
    "        \n",
    "    path_NetCDF_chla=''.join([dir_data_chla,'/',str(y),str(M),'01_m_',str(y),str(M),str(fin_day),'-ACRI-L4-CHL-MULTI_4KM-GLO-REP.nc'])\n",
    "    print(path_NetCDF_chla)  \n",
    "        \n",
    "    path_NetCDF_par =''.join([dir_data_par,'/L3m_',str(y),str(M),'01-',str(y),str(M),str(fin_day),'__GLOB_4_AVW-MODVIR_PAR_MO_00.nc'])\n",
    "    print(path_NetCDF_par)         \n",
    "        \n",
    "    path_NetCDF_sla=''.join([dir_data_sla,'/', 'dt_global_allsat_msla_h_y', str(y),'_m',str(M),'.nc'])\n",
    "    print(path_NetCDF_sla)\n",
    "    #####################################################################\n",
    "    \n",
    "    rrs412 = load_netcdf(path=path_NetCDF_rrs412, var=\"RRS412\")[0]\n",
    "    lon_oc = load_netcdf(path=path_NetCDF_rrs412, var=\"RRS412\")[1]\n",
    "    lat_oc = load_netcdf(path=path_NetCDF_rrs412, var=\"RRS412\")[2]\n",
    "    \n",
    "    rrs443 = load_netcdf(path=path_NetCDF_rrs443, var=\"RRS443\")[0]\n",
    "       \n",
    "    rrs490 = load_netcdf(path=path_NetCDF_rrs490, var=\"RRS490\")[0]\n",
    "        \n",
    "    rrs555 = load_netcdf(path=path_NetCDF_rrs555, var=\"RRS555\")[0]\n",
    "        \n",
    "    rrs670 = load_netcdf(path=path_NetCDF_rrs670, var=\"RRS670\")[0]       \n",
    "    \n",
    "    chla = load_netcdf(path=path_NetCDF_chla, var=\"CHL\")[0]        \n",
    "    \n",
    "    par = load_netcdf(path=path_NetCDF_par, var=\"PAR_mean\")[0]\n",
    "    \n",
    "    \n",
    "    sla = load_netcdf(path=path_NetCDF_sla, var=\"sla\")[0]\n",
    "    lon_sla = load_netcdf(path=path_NetCDF_sla, var=\"sla\")[1]\n",
    "    lat_sla = load_netcdf(path=path_NetCDF_sla, var=\"sla\")[2]\n",
    "        \n",
    "    \n",
    "       \n",
    "        \n",
    "    #Create VAR3D matrix which is the matrix in which we will have the cube retrieval of Chl from Uitz et al 2006\n",
    "    SOCA_CHLA_3D=np.empty((len(CMEMS_pres_N),len(lat_phy),len(lon_phy2)))\n",
    "    SOCA_CHLA_3D[:,:,:]=np.nan\n",
    "    #Create CHL3D ERROR matrix which is the matrix in which we will have the cube error of retrieval of Chl from Uitz et al 2006\n",
    "    SOCA_CHLA_3D_ERR=np.empty((len(CMEMS_pres_N),len(lat_phy),len(lon_phy2)))\n",
    "    SOCA_CHLA_3D_ERR[:,:,:]=np.nan\n",
    "\n",
    "\n",
    "    #Loop for each pixel (lon/lat phy)\n",
    "    for i in np.arange(len(lon_phy2)):\n",
    "        print(i)\n",
    "        warnings.filterwarnings('ignore') # Ignore warning messages for printing\n",
    "        #iii_oc the location of the pixel of ocean color that match with physical longitude\n",
    "        iii_oc=np.logical_and(lon_oc >= lon_phy2[i]-0.125, lon_oc <= lon_phy2[i]+0.125)\n",
    "        iii_oc=np.where(iii_oc)\n",
    "        iii_oc=np.array(iii_oc)[0,:]\n",
    "            \n",
    "        #iii_sla is the location of the pixel of sla that match with physical longitude\n",
    "        iii_sla=np.logical_and(lon_sla >= lon_phy2[i]-0.125, lon_sla <= lon_phy2[i]+0.125)\n",
    "        iii_sla=np.where(iii_sla)\n",
    "        iii_sla=np.array(iii_sla)[0,:]\n",
    "\n",
    "        for j in np.arange(len(lat_phy)):\n",
    "            #jjj_oc is the location of the pixel of ocean color that match with physical latitude\n",
    "            jjj_oc=np.logical_and(lat_oc >= lat_phy[j]-0.125, lat_oc <= lat_phy[j]+0.125)\n",
    "            jjj_oc=np.where(jjj_oc)\n",
    "            jjj_oc=np.array(jjj_oc)[0,:]\n",
    "                \n",
    "            #jjj_sla is the location of the pixel of sla that match with physical latitude\n",
    "            jjj_sla=np.logical_and(lat_sla >= lat_phy[j]-0.125, lat_sla <= lat_phy[j]+0.125)\n",
    "            jjj_sla=np.where(jjj_sla)\n",
    "            jjj_sla=np.array(jjj_sla)[0,:]\n",
    "                \n",
    "            # Apply the Black Sea mask:\n",
    "            black_sea=mask_black_sea(lon=lon_phy2[i], lat=lat_phy[j])   \n",
    "            # Apply the bathymetric mask <ith threshold 1500m depth:\n",
    "            bathy=mask_bathy(bathy_mat=Height, lon_bathy=Lon_bathy, lat_bathy=Lat_bathy, lon=lon_phy2[i], lat=lat_phy[j], threshold=-1500)\n",
    "            if not black_sea and bathy:  \n",
    "#                    print(\"ok\")\n",
    "                #Get the MLD and Chl value for Chl vertical distribution\n",
    "                mld_soca=mlotst[j,i]\n",
    "#                     chl_sat_uitz=get_mean_pixel(VAR_MAT=chl, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "\n",
    "                # Get the temperature and salinity profiles\n",
    "                temp_soca = to[:,j,i]\n",
    "                sal_soca = so[:,j,i]\n",
    "                \n",
    "             \n",
    "                # Get the RRS values and SLA and PAR\n",
    "                rrs412_soca=get_mean_pixel(VAR_MAT=rrs412, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs443_soca=get_mean_pixel(VAR_MAT=rrs443, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs490_soca=get_mean_pixel(VAR_MAT=rrs490, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs555_soca=get_mean_pixel(VAR_MAT=rrs555, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                rrs670_soca=get_mean_pixel(VAR_MAT=rrs670, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                par_soca=get_mean_pixel(VAR_MAT=par, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                sla_soca=get_mean_pixel(VAR_MAT=sla, lon_ix=iii_sla, lat_ix=jjj_sla)\n",
    "                chla_matchup = get_mean_pixel(VAR_MAT=chla, lon_ix=iii_oc, lat_ix=jjj_oc)\n",
    "                ZNORM=ZNORM_N(chla_matchup,mld_soca)\n",
    "                \n",
    "\n",
    "\n",
    "                    # Condition also on temp and sal because sometimes some depths are masked and not others so mld is not nan but some values in the T/S profiles are\n",
    "#                     isinstance(mld_uitz,float) and\n",
    "                if isinstance(mld_soca,float) and all([isinstance(T, float) for T in temp_soca]) and all([isinstance(S, float) for S in sal_soca]):\n",
    "                    if not np.isnan(rrs412_soca) and not np.isnan(ZNORM) and not np.isnan(mld_soca) and not np.isnan(rrs443_soca) and not np.isnan(rrs490_soca) and not np.isnan(rrs555_soca) and not np.isnan(rrs670_soca) and not np.isnan(sla_soca) and not np.isnan(par_soca):\n",
    "                        \n",
    "                        ############ DERIVE SPICINESS ######################################\n",
    "                        spici_soca = swe.spice(pd.DataFrame(sal_soca), pd.DataFrame(temp_soca), pd.DataFrame(CMEMS_pres_N))\n",
    "                        spici_soca = np.squeeze(spici_soca)\n",
    "                \n",
    "                        ################## DERIVE BRUNT VAISALA FREQUENCY###################\n",
    "                        #Compute Brünt-Väisälä Frequency squared (N²)\n",
    "                        # Compute first the absolute salinity and conservative temperature\n",
    "#                         Absolute_Sal = gsw.SA_from_SP(sal_soca,temp_soca,lon=lon_phy2[i], lat=lat_phy[j]) #help(gsw.SA_from_SP)\n",
    "#                         Conservative_temp = gsw.CT_from_t(sal_soca, temp_soca, CMEMS_pres)\n",
    "#                         # N squared and pres associated\n",
    "#                         N2 = gsw.Nsquared(Absolute_Sal, Conservative_temp, CMEMS_pres, lat=lat_phy[j])[0]\n",
    "#                         p_N2 = gsw.Nsquared(Absolute_Sal, Conservative_temp, CMEMS_pres, lat=lat_phy[j])[1]\n",
    "#                         # # Then interpolate to have the good pressure \n",
    "#                         interp1=interp1d(p_N2,N2,kind='nearest',fill_value=\"extrapolate\")\n",
    "#                         brunt_soca=interp1(CMEMS_pres)\n",
    "                        \n",
    "                                                \n",
    "                        Zeta_interp = np.linspace(0,1.5,50)\n",
    "                        Zeta = CMEMS_pres_N/ZNORM\n",
    "                        temp_soca_ZNORM = np.interp(Zeta_interp, Zeta,temp_soca)\n",
    "                        sal_soca_ZNORM = np.interp(Zeta_interp, Zeta,sal_soca)\n",
    "                        spici_soca_ZNORM = np.interp(Zeta_interp, Zeta,spici_soca)\n",
    "#                         brunt_soca_ZNORM = np.interp(Zeta_interp, Zeta,brunt_soca)\n",
    "                        \n",
    "                                                \n",
    "                        temp_soca_ZNORM = temp_soca_ZNORM.reshape(-1,1)\n",
    "                        sal_soca_ZNORM = sal_soca_ZNORM.reshape(-1,1)\n",
    "                        spici_soca_ZNORM = spici_soca_ZNORM.reshape(-1,1)\n",
    "                                                \n",
    "                \n",
    "                        #SOCA2020 retrieval\n",
    "                        soca_chla_2020_inputs=INPUTS_SOCA_CHLA_2020(RHO_WN_412=rrs412_soca*3.14, RHO_WN_443= rrs443_soca*3.14,\n",
    "                                                                    RHO_WN_490=rrs490_soca*3.14, RHO_WN_555= rrs555_soca*3.14, RHO_WN_670= rrs670_soca*3.14,\n",
    "                                                                    SLA= sla_soca, PAR=par_soca, MLD=mld_soca,sal= sal_soca_ZNORM, temp=temp_soca_ZNORM, spici=spici_soca_ZNORM,\n",
    "                                                                    lon=lon_phy2[i], lat=lat_phy[j], doy=doy_f)\n",
    "\n",
    "                        chla_soca = SOCA_CHLA_ZNORM_2020(soca_chla_2020_inputs,ZNORM,pres_new=CMEMS_pres_N)[0]\n",
    "                        chla_soca_err = SOCA_CHLA_ZNORM_2020(soca_chla_2020_inputs,ZNORM,pres_new=CMEMS_pres_N)[1]\n",
    "\n",
    "#                             POC_soca = POC_from_bbp(bbp_soca, bbp_soca_err)[0]\n",
    "#                             POC_soca_err = POC_from_bbp(bbp_soca, bbp_soca_err)[1]\n",
    "\n",
    "                            # Fill POC3D with SOCA2020 retrieval\n",
    "    \n",
    "                        SOCA_CHLA_3D[:,j,i]=chla_soca\n",
    "                        SOCA_CHLA_3D_ERR[:,j,i]=chla_soca_err\n",
    "#                             SOCA_CHLA_3D_N[:,j,i]=chla_soca\n",
    "#                             SOCA_CHLA_3D_ERR_N[:,j,i]=chla_soca_err\n",
    "            \n",
    "    date1=f.split(\"_\")[1][:8]\n",
    "    DATE_PRODUCT=date1\n",
    "    LONG=lon_phy2\n",
    "    print(LONG.shape)\n",
    "    LAT=np.array(lat_phy,)\n",
    "    print(LAT.shape)\n",
    "    DEPTH=np.array(CMEMS_pres_N)\n",
    "    DEPTH=np.array(DEPTH,dtype=\"int16\")\n",
    "    print(DEPTH.shape)\n",
    "    VAR4D=np.expand_dims(SOCA_CHLA_3D, axis=0)\n",
    "    print(VAR4D.shape)\n",
    "    VAR4D_ERR=np.expand_dims(SOCA_CHLA_3D_ERR, axis=0)\n",
    "    print(VAR4D_ERR.shape)\n",
    "#         VAR4D2=np.expand_dims(SOCA_CHLA_3D_N, axis=0)\n",
    "#         VAR4D_ERR2=np.expand_dims(SOCA_CHLA_3D_ERR_N, axis=0)\n",
    "        #Creation of the NetCDF\n",
    "    creation_NetCDF_3D_PRODUCT_CMEMS_N(path=path_nc_tac_mob, date_today= DATE_TODAY, date_product=DATE_PRODUCT,depth_input=DEPTH,longitude_input=LONG, latitude_input=LAT,variable_4d_input1=VAR4D,variable_4d_error_input1=VAR4D_ERR) \n",
    "#                                          variable_4d_input2=VAR4D2,variable_4d_error_input2=VAR4D_ERR2)\n",
    "        #End time to compute the time to create the NetCDF\n",
    "    time2=datetime.now()\n",
    "    #Compute the time\n",
    "    difference =time2-time1\n",
    "    print(difference)\n",
    "#     text_file = open(''.join([str(path_TACMOB),str(f),'_time_computation.txt']), \"w\")\n",
    "#     n = text_file.write(str(difference))\n",
    "#     text_file.close()\n",
    "########################\n",
    "########################       \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
